
Machine learning privacy preservation methods are categorized into two main approaches. Cryptographic approach applies to the scenarios where the data owners do not wish to expose their plain-text sensitive data while asking for machine learning services from a third-party. The most common tool used in this approach is fully homomorphic encryption that supports multiplication and addition operations over encrypted data, which enabling the ability to perform a more complex function. However, the high cost of the multiplicative homomorphic operations renders it difficult to be applied on machine learning tasks. In order to avoid multiplicative homomorphic operations, additive homomorphic encryption schemes are more widely used in privacy preserving machine learning (PPML). However, the limitation of the computational capacity in additive homomorphic schemes narrows the ability to apply on particular ML techniques. Thus, such additive homomorphic encryption-based methods in \cite{Bost2015,Emekci2007,Raphael,Hesamifard}  are only applicable to simple machine learning algorithms such as decision tree and naive bayes. In Hesamifard's work \cite{Hesamifard2017},the fully homomorphic encryption is applied to perform deep neural networks over encrypted data, where the non-linear activation functions are approximated by polynomials. 
  
In secure multi-party computing (SMC), multiple parties collaborate to compute functions without revealing plain-text to other parties. A widely-used tool in SMC is garbled circuit \cite{Yao1986}, a cryptographic protocol carefully designed for two-party computation, in which they can jointly evaluate a function over their sensitive data without the trust of each other. In \cite{Al-rubaie}, Mohammad introduced a SMC protocol for principle component analysis (PCA) which is a hybrid system utilizing additive homomorphic and garbled circuit. In secret sharing techniques \cite{Shamir1979}, a secret \textbf{s} is distributed over multiple pieces \textbf{n} also called \textit{shares}, where the secret can only be recovered by a sufficient amount of \textbf{t} \textit{shares}. A good review of secret sharing-based techniques and encryption-based techniques for PPML is given in \cite{Pedersen2007}. Although these encryption-based techniques can protect the privacy in particular scenarios, their computational cost is a significant concern. Furthermore, as \cite{Pedersen2007} elaborated, the high communication cost also poses a big concern for both techniques.
      
The other category is Non-Cryptographic approach. For example, Differential Privacy (DP) \cite{Dwork2006} aims to prevent membership inference attacks. DP considers a scenario that an adversary infers a member's information based on the difference of outputs of a ML mechanism before and after the member join a database. The database with the member's information and without the member's information can be considered as two neighbor databases which differ by at most one element. DP adds noise to the outputs of the ML mechanism to result in similar outputs from the two neighbor databases. Thus, adversaries cannot differentiate the difference between the two databases. A mechanism M satisfies $\epsilon$-differential privacy if for any two neighbor databases $D$ and $D'$, and any subset S of the output space of M satisfies 
$ Pr[M(D) \in S] \leq e^{\epsilon}  Pr[M(D') \in S]  $. 
The similarity of query outputs protects a member information from such membership inference attacks. The \textit{similarity} is guaranteed by the parameter $\epsilon$ in a mechanism in which the smaller $\epsilon$ provides a better level of privacy preservation. \cite{Chaudhuri, Zhang2012,NIPS2008, Wu, MYang} propose methods to guarantee $\epsilon$-differential privacy by adding noise to outcome of the weights $w^*= w + \eta$, where $\eta$ drawn from Laplacian distribution and adding noise to the objective function of logistic regression or linear regression models. \cite{Phan2016,Abadi2016} satisfy differential privacy by adding noise to the objective function while training a deep neural network using stochastic gradient descent as the optimization algorithm. 

In addition, there are existing works proposing differential privacy dimension reduction. One can guarantee $\epsilon$-differential privacy by perturbing dimension reduction outcome. Principal component analysis (PCA) whose output is a set of eigenvectors is a popular method in dimension reduction. The original data is then represented by its projection on those eigenvectors, which keeps the largest variance of the data. One can reduce the data dimension by eliminating insignificant eigenvectors which contain less variance, and apply noise on the outcome to achieve differential privacy\cite{XiaoqianJiangZhanglongJiShuangWangNomanMohammedSamuelCheng2013}. However, the downside of these methods is that they are designed for specific mechanisms and datasets and not working well with the others. For example, record-level differential privacy is not effectively used with image dataset as shown in \cite{Hitaj2017}. Also, the amount of added noise is accumulative based on the number of queries so that this approach usually leads to low accuracy results with a high number of queries. 

Similar to our work, Generative Adversarial Privacy (GAP) \cite{GAP} is a perturbation method utilizing the minimax algorithm of Generative Adversarial Nets to preserve privacy and to keep utility of image datasets. GAP perturbs data within a specific $l_2$ distance constraint between original and perturbed data to distort private class labels and at the same time preserve non-private class labels. However, it does not protect the images themselves, and an adversary can visually infer private label (e.g., identity) from images. In contrast, our method protects an image by compressing it into a few dimension vector and then transferring without clearly exposing the original image.

 


