

Machine Learning (ML) is an important aspect of modern applications that rely on big data analytics (e.g., an on-line system collecting data from multiple data owners). However, these applications are progressively raising many different privacy issues as they collect different types of data on a daily basis. For example, many types of data are being collected in smart cities such as patient records, salary information, biological characteristics, Internet access history, personal images and so on. These types of data then can be widely used in daily recommendation systems, business data analysis, or disease prediction systems which in turn affect the privacy of individuals who contributed their sensitive data. Considering a multi-level access control system of a company using biometric recognition (e.g., face recognition, fingerprint) for granting permission to access data resources, the company staff members may concern their biological information being vulnerable to adversaries. Even though the utility of these biometric features can be effectively used in machine learning tasks for authentication purpose, leaking this information might lead to privacy breaches. For example, an adversary could utilize them to determine the members' identities.

Several tools and methods have been developed to preserve the privacy in machine learning applications, such as homomorphic encryption \cite{Bost2015,Emekci2007,Hesamifard2017}, secure multi-party computing \cite{Yao1986,Shamir1979}, differential privacy (DP) \cite{Chaudhuri,Zhang2012,Phan2016,Abadi2016,XiaoqianJiangZhanglongJiShuangWangNomanMohammedSamuelCheng2013}, compressive privacy \cite{zhuang2017fripal,GAP,DBLP:journals/corr/abs-1809-08911,DifferentialPrivacywithCompression,7815484, Kung, XieKun} and so on. Typically, differential privacy-based methods aim at preventing leaking individual information caused by queries. However, they are not designed to serve large number of queries since they require adding huge amount of noise to preserve privacy, thus significantly decreasing the ability to learn meaningful information from data. On the other hand, homomorphic encryption-based methods can be used to privately evaluate a function over encrypted data by a third party without accessing to plain-text data, hence the privacy of data owners can be protected. However, due to the high computational cost and time consumption, they may not work with a very large dataset, normally required in ML applications.

In this study, we consider an access control system collecting dimension-reduced face images of staff members to perform authentication task and to provide permission for members who would like to access company's data resources (Figure \ref{fig:attackmodel}). We propose a non-linear dimension reduction framework to decrease data dimension for the authentication purpose mentioned above and to protect against an adversary from reconstructing member images. Firstly, we introduce $\epsilon$-DR Privacy as a theoretical tool for dimension reduction privacy evaluation. It evaluates the reconstruction distance between original data and reconstructed data of a dimension reduction (DR) mechanism. This approach encourages a DR mechanism to enlarge the distance as high distance yields high level of privacy. While other methods such as differential privacy-based methods rely on inference uncertainty to protect sensitive data, $\epsilon$-DR Privacy is built on reconstruction error to evaluate privacy. Therefore, unlike differential privacy methods,  $\epsilon$-DR Privacy is not negatively impacted by the number of queries. Secondly, as detailed in Section \ref{sec:autogan_methodology}, we recommend a privacy-preserving framework Autoencoder Generative Adversarial Nets-based Dimension Reduction Privacy (AutoGAN-DRP) for enhancing data owner privacy and preserving data utility. The \textit{utility} herein is evaluated via machine learning task performance (e.g., classification accuracy).

Our dimension reduction (DR) framework can be applied to different types of data and used in several practical applications without heavy computation of encryption and impact of query number. The proposed framework can be applied directly to the access control system mentioned above. More elaboratively, face images are locally collected, nonlinearly compressed to achieve DR, and sent to the authentication center. The server then performs classification tasks on the dimension-reduced data. We assume the authentication server is semi-honest, that is to say it does not deviate from authenticating protocols while being curious about a specific member's identity. Our DR framework is designed to resist against reconstruction attacks from a strong adversary who obtains the training dataset and the transformation model. 

During the stage of experiments, we implemented our framework to evaluate dimension-reduced data in terms of accuracy of the classification tasks, and we attempted to reconstruct original images to examine the capacity of adversaries. We performed several experiments on three facial image datasets in both gray-scale and color, i.e., \textit{the Extended Yale Face Database B} \cite{GeBeKr01}, \textit{AT}\&\textit{T} \cite{341300}, and \textit{CelebFaces Attributes Dataset (CelebA)} \cite{celeba}. The experiment results illustrate that with only seven reduced dimensions our method can achieve accuracies of 93\%, 90\%, and 80\% for AT\&T, YaleB, and CelebA respectively. Further, our experiments show that at the accuracies of 79\%, 80\% and 73\% respectively, the reconstructed images could not be recognized by human eyes. In addition, the comparisons shown in Section \ref{sec:AutoGAN_DP_PCA} also illustrate that AutoGAN-DRP is more resilient to reconstruction attacks compared to related works.
Our work has two main contributions:
\begin{enumerate}
\item To analytically support privacy guarantee, we introduce $\epsilon$-DR Privacy as a theoretical approach to evaluate privacy preserving mechanism.
\item We propose a non-linear dimension reduction framework for privacy preservation motivated by Generative Adversarial Nets \cite{Goodfellow2014} and Auto-encoder Nets \cite{Baldi2012}. 
  
\end{enumerate}
The rest of this chapter is organized as follows. Section \ref{sec:autogan_relatedwork} summarizes state-of-the-art privacy preservation machine learning (PPML) techniques and reviews knowledge of deep learning methods including generative adversarial neural nets and Auto-encoder. Section \ref{sec:autogan_methodology} describes the privacy problem through a scenario of a facial recognition access control system, introduces the definition of $\epsilon$-DR Privacy to evaluate DR-based privacy preserving mechanisms, and presents our framework \Name. Section \ref{sec:experimentsanddiscussion} presents and discusses our experiment results over three different face image datasets. Section \ref{sec:AutoGAN_GAP} compares AutoGAN-DRP to a similar work GAP in terms of reconstruction error and classification accuracy. Section \ref{sec:AutoGAN_DP_PCA} demonstrates reconstructed images over AutoGAN-DRP and other privacy preservation techniques (i.e., Differential Privacy and Principle Component Analysis). Finally, the conclusion and future work are mentioned in Section \ref{sec:Autogan_conclusion}. 
