


\chapter{Introduction}
\label{chap:Intro}

In recent decades, deep learning has become successful in many automatic tasks such as pattern recognition, natural language processing, image and vision computing by extracting and learning from large amounts of data. While most existing works focus on enhancing machine learning models to improve task performance, data quality improvement is still in lacks of attention. There are numerous aspects to look for in order to improve data quality, such as data augmentation, data dimensionality reduction, data imbalance, missing data, and feature extraction. In practice, by improving data quality, we could significantly boost machine learning performance. In this work, we aim to enhance machine learning performance by focusing on data processing techniques to alleviate the negative impacts of class imbalance, data that is not independent and identically distributed (non-IID). Besides, we also propose a deep learning-based dimensionality reduction technique to improve classification accuracy and preserve user privacy simultaneously. Proposed techniques are introduced in the following chapters. Specifically, Chapter \ref{chap:FedDisk} introduces a method to tackle the non-IID issue in federated learning, which might significantly reduce machine learning performance. Chapter \ref{chap:SIMPOR} introduces a class balancing technique that generates synthetic data for minor classes. This could help deep learning to avoid slow convergence problems and task performance. In Chapter \ref{chap:AUTOGAN}, we propose a dimensionality reduction technique based on the state-of-the-art generative models (i.e., AutoEncoder and Generative Adversarial Network) to not only improve classification accuracy but also preserve data privacy. 
