
\section{Related Work}
\label{sec:relatedwork}
In the last few decades, there have been a number of solutions proposed to alleviate the negative impacts of data imbalance in machine learning. However, many of them are not efficient when it comes to high-dimensional data and deep learning. In this section, we review algorithms that aim at deep learning and strategies inherited from conventional machine learning methods. These techniques can mainly be categorized into two different categories, i.e., data-centric and model-centric approaches. 


Model-centric approaches usually require modifications of algorithms on the cost functions in order to balance the weight of each class. Specifically, such cost-sensitive approaches put higher penalties on majority classes and less on minority classes to balance their contribution to the final cost. For example,  \cite{cui_class-balanced_2019} provided their designed formula $ (1 - \beta^n)/(1 - \beta)$ to compute the weight of each class based on the effective number of samples $n$ and a hyperparameter $\beta$ which is then applied to re-balance the loss of a convolutional neural network model.  \cite{huang_learning_2016},\cite{rangarajan_sridhar_unsupervised_2015}, \cite{DBLP:journals/corr/abs-1805-00932} assign classes' weights inversely proportional to sample frequency appearing in each class.      


Compared to model-centric-based manners, data-centric approaches have been attracting more research attention as it is independent of machine learning algorithms. In this category, we divide into two main approaches, i.e. sampling-based and generative approaches. Sampling-based methods \cite{DBLP:journals/corr/ShenLH15}, \cite{DBLP:journals/corr/abs-1711-00941}, \cite{haibo_he_learning_2009}, \cite{li_entropy-based_2020}, \cite{ertekin_active_2007} mainly generate a balanced dataset by either over-sampling minority classes or down-sampling majority classes. Some methods are not designed for deep learning, but we still consider them since they are independent of the machine learning model architecture. In a widely used method SMOTE \cite{chawla_smote:_2002}, Chawla \textit{et al.} attempt to oversampling minority class samples by connecting an sample to its neighbors in feature space and arbitrarily drawing synthetic samples along the connections. However, one of the drawbacks of SMOTE is that if there are samples in the minority class located in the majority class, it creates synthetic sample bridges towards the majority class \cite{goswami_class_2020}. This renders difficulties in differentiation between the two classes. Another SMOTE-based work namely Borderline-SMOTE \cite{bordersmote} was proposed in which its method aims to do SMOTE with only samples near the border between classes. The samples near the border are determined by the labels of its \textit{k} distance-based neighbors (if more than half of neighbors belong to the other class, the sample is considered to be on the border). This "border" idea is similar to ours to some degree. However, finding a good \textit{k} is critical, and it is usually highly data-dependent. In addition, Borderline-SMOTE again faces the problems of SMOTE. 


Under the down-sampling category, other works \cite{ertekin_learning_2007}, \cite{aggarwal_active_2020} leverage active learning techniques to find informative samples which authors believe the imbalance ratio in these areas is much smaller than that in the entire dataset. They then classify this small pool of samples to improve the performance and expedite the training process for the SVM-based method. however, this method was only designed for SVM-based methods which mainly depend on the support vectors. Also, this potentially discards important information of the entire dataset because only a small pool of data is used.  

Generative approaches which generate synthetic samples in minor classes by sampling from data distribution are becoming more attractive as they are outperforming other methods in high dimensional data \cite{DBLP:conf/dmin/LiuGM07}. When it comes to images, a number of deep learning generative-based methods have been proposed as deep learning is capable of capturing good image representations. \cite{rashid_convergence_2012} \cite{dai_generative_2019} \cite{mullick_generative_2019} utilized Variational Autoencoder as a generative model to arbitrarily generate images from learned distributions. However, most of them assumed simple prior distributions such as Gaussian for minor classes, they tend to simplify data distribution and might not succeed in sophisticated distributions. Our solution also falls into this category; however, we leverage the idea of a mixture model to tackle this issue for image data.     