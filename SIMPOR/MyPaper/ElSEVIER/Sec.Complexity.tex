

\section{Algorithm Implementation and Complexity}
\label{sec:implementation}
Our proposed method is straightforward in implementation. We first train a neural network model with initial samples and start querying next batches data based on the entropy scores from previous model to find informative samples. The model is then updated with new batches of data until the entropy scores reach a certain threshold. All the informative samples are then balanced first, and the remaining data are balanced later. Each synthetic data point is generated by finding a local maxima in Equation \ref{equ:f}.

Perhaps the costly part of \Methodname{} is that each synthetic sample requires to compute a kernel density estimation of the entire dataset. Elaborately, let $n$ be the number of samples of the dataset. In the worst case, the number of samples of minority and majority class are $N_B = 1$ and $N_A = n-2$ respectively. We need to generate $n-1$ synthetic samples to completely balance the dataset. Since each generated sample must loop through the entire dataset of size $n$ to estimate the density, the complexity is $O(n^2)$. 

Although generating synthetic data is only a one-time process, and this does not affect the classification performance in the testing phase, we still alleviate its weakness by providing parallelized implementations. We provide two suggestions, multiple CPU thread-based and GPU-based implementations. While the former simply computes each synthetic data sample in a separated CPU thread, the later computes each exponential component in \ref{equ:f} parallelly in GPU's threads. More specifically, Equation \ref{equ:f} can be rewritten as $N_B$ components of $e^{\frac{1}{2} (\frac {x - X_{B_i}}{h})^2}$ and $N_A$ components of $e^{\frac{1}{2} (\frac {x - X_{A_i}}{h})^2}$. Fortunately, they are all independent and can be parallelly processed in GPUs. The latter is then implemented using Python Numba and Cupy libraries which utilize CUDA toolkit from NVIDIA \cite{cuda}. The consumption time for kernel density estimation for each synthetic data point is then reduced by $N_A+N_B=n$ times, which significantly simplifies the complexity to $O(n)$. Our source code can be found on following Github link  https://github.com/nsh135/SIMPOR. 