
\section{Related Work}
\label{sec:relatedwork}
In the last few decades, many solutions have been proposed to alleviate the negative impacts of data imbalance in machine learning. However, most of them are not efficiently extended for deep learning. This section reviews algorithms to tackle class-imbalanced data that can be extended for deep learning. These techniques can be categorized into three main categories, i.e., sampling, cost-sensitive, and ensemble learning approaches. 

\subsection{Sampling-based approach.}
Compared to other approaches, resampling techniques have attracted more research attention as they are independent of machine learning algorithms. This approach can be divided into two main categories, over-sampling, and under-sampling techniques. Such sampling-based methods e.g., \cite{DBLP:journals/corr/ShenLH15, DBLP:journals/corr/abs-1711-00941, haibo_he_learning_2009, li_entropy-based_2020,ertekin_active_2007,8745666,8713384} mainly generate a balanced dataset by either over-sampling the minority class or down-sampling the majority class.\R{liu} Liu \textit{et al.}, \cite{9723474, 8668459} proposed two different approaches to learn from imbalanced data by capturing critical features in minority examples using model-based and density based methods. Some techniques are not designed for deep learning; however, they are still considered in this work since they are independent of the machine learning model architecture. In a widely used method, SMOTE \cite{chawla_smote:_2002}, Chawla \textit{et al.} attempt to oversample minority class samples by connecting a sample to its neighbors in feature space and arbitrarily drawing synthetic samples along with the connections. However, one of SMOTE drawbacks is that if there are samples in the minority class located in the majority class, it creates synthetic sample bridges toward the majority class \cite{goswami_class_2020}. This renders difficulties in differentiation between the two classes. Another SMOTE-based work, namely Borderline-SMOTE \cite{bordersmote} was proposed in which its method aims to do SMOTE with only samples near the border between classes. The samples near the border are determined by the labels of its \textit{k} distance-based neighbors. This "border" idea is similar to ours to some degree. However, finding a good \textit{k} is critical for a heuristic machine learning algorithm such as deep learning, and it is usually highly data-dependent. 

Among specific techniques for deep learning, generating synthetic samples in the minority class by sampling from data distribution is becoming more attractive as they outperform other methods in high dimensional data \cite{DBLP:conf/dmin/LiuGM07}. Regarding images, several deep learning generative-based methods have been proposed as deep learning is capable of capturing good image representations. \cite{rashid_convergence_2012} \cite{dai_generative_2019} \cite{mullick_generative_2019} utilized Variational Autoencoder as a generative model to arbitrarily generate images from learned distributions. However, most assumed simple prior distributions, such as Gaussian for minor classes, tend to simplify data distribution and might fail in more sophisticated distributions. In addition, most of the works in this approach tackle image datasets, while our proposed method focuses on tabular datasets as this is a missing piece in the field \cite{johnson_survey_2019}.

Under the down-sampling category, existing techniques mainly down-sample the majority class to balance it with the minority class. There are several proposed techniques to simplify the majority. A straightforward way is to randomly remove the majority class samples. Other works, e.g., \cite{ertekin_learning_2007}, \cite{aggarwal_active_2020} find near-border samples and authors believe the imbalance ratio in these areas is much smaller than that in the entire dataset. They then classify this small pool of samples to improve the performance and expedite the training process for the SVM-based method. However, this method was only designed for SVM-based methods, which mainly depend on the support vectors. Also, this potentially discards essential information of the entire dataset because only a small pool of data is used.  

\subsection{Cost-sensitive learning approach.}
Cost-sensitive learning techniques usually require modifications of algorithms on the cost functions to balance each class's weight. Specifically, such cost-sensitive techniques put higher penalties on majority classes and less on minority classes to balance their contribution to the final cost. For example,  \cite{cui_class-balanced_2019} provided their designed formula $ (1 - \beta^n)/(1 - \beta)$ to compute the weight of each class based on the effective number of samples $n$ and a hyperparameter $\beta$ which is then applied to re-balance the loss of a convolutional neural network model.  \cite{huang_learning_2016},\cite{rangarajan_sridhar_unsupervised_2015}, \cite{DBLP:journals/corr/abs-1805-00932} assign classes' weights inversely proportional to sample frequency appearing in each class. Hamed et al. \cite{cssvm} proposed an SVM-based cost-sensitive approach (SVMCS) that uses svm with a class-weighted loss function.  
  
\subsection{Ensemple learning approach.}
Ensemble learning has achieved high performance in classification for its generalizability. Thus, it could reduce the bias due to class imbalance. Ensemble learning can be constructed by combining several base classifiers with different sampling-based approaches. In \cite{SMOTEBoost, RUSBoost}, Chawla \textit{et al.} and Seiffert \textit{et al.} proposed variants of ensemble learning in which the data are balanced based on oversampling method SMOTE and then applying ensemble learning on balanced data. Similarly, authors in \cite{ECOEnsemble} generate cluster-based synthetic data and combine it with an evolutionary algorithm. Liu \textit{et al.} in \cite{LIU201735} balances the data by applying a fuzzy-based oversampling technique and building ensemble learning classifiers on this data. Zhou and Liu in \cite{EE} explore a method, namely Easy Ensemble classifier (EE), to perform ensemble learning on the random under-sampling balanced data.
     
