\clipboard {non-IID}{ One of the most challenging issues in federated learning is that the data is often not independent and identically distributed (non-IID). Clients are expected to contribute the same type of data and drawn from one global distribution. However, data are often collected in different ways from different resources. Thus, the data distributions among clients might be different from the underlying global distribution. This creates a weight divergence issue and reduces federated learning performance. This work focuses on improving federated learning performance for skewed data distribution across clients.}
\clipboard {para:motivation}{The primary problem is the divergence of model weights, as found in \cite {Zhao2018FederatedLW} by Zhao et al. The authors showed that the model's weights tend to be more diverged for non-IID data compared to that for IDD data. This causes a performance reduction, and it worsens as the data distribution becomes more skewed. For example, the accuracy dropped by about 10\% for image dataset Cifar-10 \cite {cifar10}, and speech recognition dataset KWS \cite {kws} with a non-IID setting}
\clipboard {sec:introduction}{To address this problem, we proposed an algorithm that utilizes sample weights to adjust individual client distributions closer to the global distribution during the training process. However, obtaining global information across clients is challenging in an FL setting because clients need to allow the exposure of their raw data. To overcome this challenge, the proposed method implicitly shares statistical information of client data without revealing the client's raw data. The method only requires clients to exchange additional model weights using a typical FL procedure. Once the adjustment weights are acquired, the machine learning model can be trained using a standard FL framework. The proposed method is demonstrated to improve FL accuracy and significantly reduce FL communication costs through experiments on three real-world datasets. \par Our contributions are as follows: \begin {enumerate} \item Provide a theoretical base for skewed feature distribution data for federated learning by adjusting sample weights derived from the machine learning empirical risk. \item Provide a practical solution to mitigate the problem of learning from non-IID data for the FL framework without sharing clients' draw data. The proposed method only requires clients to share additional model parameters, similar to a typical federated learning framework. \item Several experiments were conducted on three datasets, including MNIST, non-IID benchmark dataset FEMNIST and real-world dataset Chest-Xray. The results demonstrate that the proposed method outperforms other experimental methods in classification accuracy and dramatically reduces the communication cost. \item As the proposed method needs to exchange additional information, we also provide a theoretical analysis to analyze the potential privacy leakage. We showed that the leakage information becomes insignificant when the number of clients increases. \item To our best knowledge, the proposed method is the first method utilizing data distribution information and sample weights to tackle the FL Non-IID issue. \end {enumerate} }
\clipboard {par:problemStatement}{ \section {Problem Statement} \label {sec:problem} In this section, we introduce and formulate the scenario of FL with skewed feature distribution across clients. Our scenario is a learning collaboration between $K$ clients to build a global classification model that maximizes the global accuracy given arbitrary data. Each client holds a number of individual records that they are not willing to share with others due to privacy concerns. This study focuses on preventing the performance of the global model from deteriorating because of the distribution skewness issue \cite {survey} across clients. \par Our goal is to adjust the clients' distributions to be closer to the global distribution via sample weights. We denote the data and associated labels held by client $k$ $\in $ $\{1,...,K\}$ as $ \{( \mathbf {x}_k^j,y_k^j )\}_{j=1}^{N_k}$ where $ \mathbf {x}_k^j \in \mathbb {R}^d$ and $y_k^j \in \mathbb {N}$. Let the data distribution of the $k^{th}$ client be $q_k(\mathbf {x})$ and the ground truth global distribution is $p(\mathbf {x})$. Our problem becomes finding an adjusting weight function for each client $k$, $\alpha (\mathbf {x}_k)$ such that \par \begin {equation} \alpha _k(\mathbf {x}) q_k(\mathbf {x}) = p(\mathbf {x}) \label {equ:goal} \end {equation} }
\clipboard {par:howEffectiveness}{ In this section, a solution is proposed to alleviate the negative impact of distribution skewness across clients for federated learning by adjusting client data distribution during the training process. The proposed method aims to find weights for training samples in order to adjust client data distributions. The remainder of this section introduces how we design sample weights. We also show that the goal in Equation \ref {equ:goal} can be derived from the machine learning optimization problem as described in this section. \par By applying the sample weights for the local training on each client, the proposed method reduces the distribution skewness of each client's data and prevents clients' raw data from being exposed. Some statistical information between clients and the aggregator must be exchanged to find sample weights. However, instead of exchanging the raw information, which might hurt clients' privacy, the proposed method only exchanges model parameters, similar to a typical Fl framework.}
\clipboard {sec:algorithm}{ \subsection {Algorithm} \label {sec:algorithm} Our main strategy can be described in Algorithm \ref {alg:FedDisk}. We only concentrated on describing the first phase of FedDisk as the second is the same as a typical FL framework. First, each client trains its own local MADE model on the local data to obtain local distribution information (lines $1-3$). All clients then jointly train global MADE utilizing a typical FL framework (lines $4-14$). After achieving these two models, data are sampled from the two output models to acquire data samples containing local and global information (lines $15-17$). These samples are concatenated with the pseudo label of $0$ for samples that come from local distribution and $1$ for ones from the global distribution (line $18$). They are then used for training an adversarial binary classifier (line $19$). The purpose is to differentiate the two datasets sampled from the two distributions. The samples that are similar to the global samples will return a higher probability of belonging to class $1$ (come from global distribution) and vice versa. Thus, the classifier probability-like output that represents class $1$ is then used to be the weights for the sample (line $20$). \par \begin {algorithm}[ht!] \caption {Phase 1: Sample Weight Computing.} \begin {flushleft} \textbf {Input}: Client $k^{th}$: Dataset \{$X_k,y_k$\}.\\ \textbf {Parameter}: $K$ : Number of client.\\ $N$ : Number of total samples.\\ $N_k$: Number of sample of client $k^{th}$. $\sum _{k=1}^{K}N_k = N$\\ $local_iter$: Number of iterations for training local model\\ $global_iter$ : Number of iterations for training global model.\\ $ld(\hat {w})$ : Local MADE model containing \textbf {l}ocal \textbf {d}istribution information\\ $gd(\tilde {w})$ : Global MADE model containing \textbf {g}lobal \textbf {d}istribution information\\ $h(\bar {w}_k)$: Shallow Binary Classifier to differentiate output from $p_x$ and $q_x$ $\alpha _k$: sample weights for client $k$ data\\ \textbf {Output}: $\alpha _k$: Sample weights for ${X_k}$ \begin {algorithmic}[1] \STATEx \COMMENT {Training local MADE model} \FOR {$k \gets $ 1 to $K$} \STATEx \hskip 0.5em ~\textbullet ~ Client $k$: \STATE Fully train $ld_k(\hat {w_k})$ on local data \{$X_k,y_k$\}. \ENDFOR \par \STATEx \STATEx \COMMENT {Training global MADE model} \FOR {$i \gets 1 $ to $global\_iter$} \FOR {$k \gets $ 1 to $K$} \STATEx \hskip 1.5em ~\textbullet ~ Client $k$: \STATE Update $\tilde {w}^{i-1}$ from the Aggregator \FOR {$j \gets 1 $ to $local\_iter$} \STATE Train $gd(\tilde {w})$ on \{$X_k,y_k$\}. \ENDFOR \STATE Sending $gd(\tilde {w})$ to Agreegator \ENDFOR \par \STATEx \hskip 0.5em ~\textbullet ~ Aggregator: \STATE \hskip 1.0em Aggregate $\tilde {w}^i = \sum _{k=1}^{K} \frac {N_k}{N} \tilde {w}^i_k$ \STATE \hskip 1.0em Broatcasting $\tilde {w}^i$ to clients \ENDFOR \par \STATEx \STATEx \COMMENT {Training shallow binary classifier} \FOR {$k \gets $ 1 to $K$} \STATEx \hskip 1.0em Sample data from local distribution:\\ \hskip 1.5em $X'^{local}_k \gets ld(X_k|\hat {w}_k)$, $y'^{local}_k \gets [0...0]$ \STATEx \hskip 1.0em Sample data from global distribution:\\ \hskip 1.5em $X'^{glob}_k \gets gd(X_k|\tilde {w}_k)$ , $y'^{global}_k \gets [1...1]$ \STATE $X'_k \gets concat(X'^{local}_k, X'^{glob}_k)$, $y'_k \gets concat(y'^{local}_k,y'^{global}_k)$ \STATEx \hskip 0.5em ~\textbullet ~ Client $k$: \STATE Fully train $h(\bar {w}_k)$ on local data \{$X'_k,y'_k$\}. \STATEx \STATEx \COMMENT {Estimate sample weight} \STATE $\alpha _k \gets h(X'_k|\bar {w})[:1]$ \ENDFOR \par \end {algorithmic} \end {flushleft} \label {alg:FedDisk} \end {algorithm} \par }
\clipboard {sec:ablation}{ \section {Ablation Study: FedDiskAb} \label {sec:ablation} In this section, we examine the idea of using sample weight for non-IID data and the FedDisk sample weight effectiveness by looking at the case when the weights are derived directly from the raw data. Specifically, instead of learning sample weights from the local and global MADE model output, the weights are learned directly from the raw local and global data. To obtain the global data, we combine all client's data and randomly sample the same number of the client dataset size to avoid data imbalance. This setting variant of FedDisk (namely FedDiskAb) is an ideal case for a sample weight-based approach as it assumes to have access to the raw data. To obtain sample weights, FedDiskAb only needs to train the binary classifier on the combination of local data and global data, aiming to discriminate the two datasets. The classifier's output is used to derive the sample weight, similar to FedDisk. \par Several experiments have been conducted for FedDiskAb and other methods in Section \ref {sec:experiments}. The outcome demonstrates that both FedDiskAb and FedDisk surpass the performance of all alternative methods. This confirms the effectiveness of the sample weight-based strategy, whether acquired through learning the distribution from MADE models or directly from the data, in enhancing federated learning when faced with non-IID challenges. Furthermore, the performance of FedDisk closely aligns with that of FedDiskAb. This shows the fact that the local and global MADE models contribute significantly to the framework's ability to capture essential distribution information, much akin to the process of direct learning from the raw data. The details of the experimental results will be shown in Chapter \ref {sec:experiments}. }
\clipboard {sec:discussion}{ \subsection {Discusion} The proposed method offers a holistic improvement over existing federated learning methods. Its combination of enhanced accuracy and reduced communication costs signifies its effectiveness across a variety of datasets and scenarios. For example, the accuray can be increased by 22\% and the communication time reduced by 8 times for FEMNIST dataset. While FedDisk exhibits remarkable performance across various metrics in federated learning scenarios, it's important to acknowledge a drawback associated with its larger model size compared to some other methods as described in Table \ref {tab:modelSize}. The increased model size leads to higher space occupation on client devices, which can have implications for light weight devices with limited storage capacity. Hence, the suggested approach could be well-suited for the cross-silo scenario, typically characterized by clients having ample data and adequate computational capabilities. Overal, the proposed approach provides an avenue to address critical challenges in federated learning, making it a promising option for real-world applications. }
